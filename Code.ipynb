{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0faab9",
   "metadata": {},
   "source": [
    "###### What do we need?\n",
    "* 1. Forward Function ✅\n",
    "* 2. Backward Function (Backpropagation) ✅\n",
    "* 3. Convolution ✅\n",
    "* 4. Pooling (MaxPooling) ✅\n",
    "* 5. Dropout ✅\n",
    "* 6. Loss (Cross-Entropy) ✅\n",
    "---------------------------------\n",
    "* 7. Optimizer (SGD) ✅\n",
    "    * SGD is simpler to implement\n",
    "---------------------------------\n",
    "* 8. Fully Connected (Dense or Linear) ✅\n",
    "* 9. Activation (ReLU & Sigmoid) ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400db5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Only cell we're going to use tensorflow --- #\n",
    "from tensorflow.keras.datasets import mnist\n",
    "# ----------------------------------------------- #\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# --- Converting raw labels to one-hot encoding --- #\n",
    "labels_dictionary = {\n",
    "    0: np.array([1,0,0,0,0,0,0,0,0,0]),\n",
    "    1: np.array([0,1,0,0,0,0,0,0,0,0]),\n",
    "    2: np.array([0,0,1,0,0,0,0,0,0,0]),\n",
    "    3: np.array([0,0,0,1,0,0,0,0,0,0]),\n",
    "    4: np.array([0,0,0,0,1,0,0,0,0,0]),\n",
    "    5: np.array([0,0,0,0,0,1,0,0,0,0]),\n",
    "    6: np.array([0,0,0,0,0,0,1,0,0,0]),\n",
    "    7: np.array([0,0,0,0,0,0,0,1,0,0]),\n",
    "    8: np.array([0,0,0,0,0,0,0,0,1,0]),\n",
    "    9: np.array([0,0,0,0,0,0,0,0,0,1])\n",
    "}\n",
    "\n",
    "new_y_train = np.zeros((y_train.shape[0], 10))\n",
    "new_y_test = np.zeros((y_test.shape[0], 10))\n",
    "for i, v in enumerate(y_train):\n",
    "    new_y_train[i] = labels_dictionary[v]\n",
    "for i, v in enumerate(y_test):\n",
    "    new_y_test[i] = labels_dictionary[v]\n",
    "# ------------------------------------------------- #.\n",
    "\n",
    "y_train = new_y_train\n",
    "y_test = new_y_test\n",
    "    \n",
    "print('X_train:', str(X_train.shape))\n",
    "print('raw y_train:', str(y_train.shape))\n",
    "print('encoded y_train:', str(new_y_train.shape))\n",
    "print(\"-----------------------------\")\n",
    "print('X_test:', str(X_test.shape))\n",
    "print('raw y_test:', str(y_test.shape))\n",
    "print('encoded y_test:', str(new_y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f54d61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Visualizing part of the mnist dataset --- #\n",
    "\n",
    "c = 300\n",
    "fig, ax = plt.subplots(3,3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax[i][j].imshow(X_train[c], cmap='gray')\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56650171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "    \n",
    "    def BinaryCrossEntropy(self):\n",
    "        # --- We'll use clip to limit values between defined min and max --- #\n",
    "        y_pred_clipped = np.clip(self.y_pred, 1e-15, 1-1e-15)\n",
    "        loss = -np.mean((self.y_true * np.log(y_pred_clipped)) + (1-self.y_true) * np.log(1-y_pred_clipped))\n",
    "        return(loss)\n",
    "    \n",
    "    def CategoricalCrossEntropy(self):\n",
    "        # --- We'll use clip to limit values between defined min and max --- #\n",
    "        y_pred_clipped = np.clip(self.y_pred, 1e-15, 1-1e-15)\n",
    "        loss = -np.mean(self.y_true*(np.log(y_pred_clipped)))\n",
    "        return(loss)\n",
    "    \n",
    "    def MeanSquaredError(self):\n",
    "        loss = np.mean(np.square(self.y_true - self.y_pred))\n",
    "        return(loss)\n",
    "    \n",
    "    def MeanAbsoluteError(self):\n",
    "        loss = np.mean(np.abs(self.y_true - self.y_pred))\n",
    "        return(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \n",
    "    def __init__(self, z):\n",
    "        self.z = z\n",
    "        \n",
    "    def ReLU(self):\n",
    "        output = np.maximum(0, self.z)\n",
    "        return(output)\n",
    "    \n",
    "    def Sigmoid(self):\n",
    "        # Used for binary classification\n",
    "        output = 1 / (1 + np.exp(-self.z))\n",
    "        return(output)\n",
    "        \n",
    "    def Softmax(self):\n",
    "        # Used for multiple classification\n",
    "        output = np.exp(self.z)/(np.sum(np.exp(self.z)))\n",
    "        return(output)\n",
    "    \n",
    "        # --- If there was an issue in runtime switch the softmax to the code bellow --- #\n",
    "        \n",
    "        # exp_shifted = np.exp(self.z - np.max(self.z))\n",
    "        # return exp_shifted / np.sum(exp_shifted)\n",
    "        \n",
    "        # ------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, params, grads, learning_rate=1e-3):\n",
    "        self.params = params\n",
    "        self.grads = grads\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def SGD(self):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.learning_rate * self.grads[i]\n",
    "        return(self.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.init = \"init function\"\n",
    "        self.cache = {}\n",
    "        \n",
    "    def XavierWeights(self, dim_in, dim_out):\n",
    "        \"\"\"Xavier initialization for weights.\"\"\"\n",
    "        \n",
    "        limit = np.sqrt(6 / (dim_in + dim_out))\n",
    "        weights = np.random.uniform(-limit, limit, (dim_out, dim_in))\n",
    "        biases = np.random.randn(dim_out, 1)\n",
    "        return(weights, biases)\n",
    "    \n",
    "    def HeWeights(self, dim_in, dim_out):\n",
    "        \"\"\"He initialization for weights.\"\"\"\n",
    "        \n",
    "        weights = np.random.randn(dim_out, dim_in) * np.sqrt(2 / dim_in)\n",
    "        biases = np.random.randn(dim_out, 1)\n",
    "        return(weights, biases)\n",
    "        \n",
    "    def Convolution(self, x, filters, kernel_size=3, stride=1, activation='relu', weight='xavier'):\n",
    "        \"\"\"'1D-Convolution'\n",
    "           Args:\n",
    "               x = input data\n",
    "               filters = number of neurons\n",
    "               kernel_size = number of kernel size\n",
    "               stride = number of stride\n",
    "               activation = default is 'relu'\n",
    "               weight = initial weight is based on 'xavier weights'\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- Weights --- #\n",
    "        w, b = self.XavierWeights(kernel_size, filters)\n",
    "        self.cache[\"weight_conv\"] = w\n",
    "        self.cache[\"bias_conv\"] = b\n",
    "        # --- ------- --- #\n",
    "        \n",
    "        # --- Output Size --- #\n",
    "        z_size = ((x.shape[0] - kernel_size) // stride) + 1\n",
    "        # --- ----------- --- #\n",
    "        \n",
    "        # --- Calculation --- #\n",
    "        self.cache['input_conv'] = x\n",
    "        z = np.zeros((filters, z_size))\n",
    "        \n",
    "        for i in range(filters):\n",
    "            for j in range(z_size):\n",
    "                segment = x[(j*stride): ((j*stride) + kernel_size)]\n",
    "                z[i][j] = np.dot(w[i], segment) + b[i][0]\n",
    "                \n",
    "        self.cache['output_conv'] = z\n",
    "        # --- ----------- --- #\n",
    "        \n",
    "        # --- Activation --- #\n",
    "        z = Activation(z).ReLU()\n",
    "        self.cache['activation_conv'] = z\n",
    "        # --- ---------- --- #\n",
    "        \n",
    "        # --- Output --- #\n",
    "        return(z)\n",
    "        \n",
    "    def MaxPooling(self, x, pool_size=2, stride=1):\n",
    "        \"\"\"'1D-MaxPooling'\n",
    "           Args:\n",
    "               x = input data\n",
    "               pool_size = number of pool size\n",
    "               kernel_size = number of kernel size\n",
    "               stride = number of stride\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- Calculation --- #\n",
    "        self.cache['input_maxpool'] = x\n",
    "        \n",
    "        z_size = ((x.shape[0] - pool_size) // stride) + 1\n",
    "        z = np.zeros(z_size)\n",
    "        for i in range(z_size):  \n",
    "            z[i] = np.max(x[i*stride: i*stride+pool_size])\n",
    "            \n",
    "        self.cache['output_maxpool'] = z\n",
    "        # --- ----------- --- #\n",
    "        \n",
    "        # --- Output --- #\n",
    "        return(z)\n",
    "        \n",
    "    def Dropout(self, x, rate=0.1):\n",
    "        \"\"\"'Dropout'\n",
    "           Args:\n",
    "               x = input data\n",
    "               rate = probablity ratio\n",
    "        \"\"\"\n",
    "        # --- Calculation --- #\n",
    "        self.cache['input_dropout'] = x\n",
    "        \n",
    "        mask = np.random.randn(x.shape[0]) > rate\n",
    "        x *= mask\n",
    "        z = x / (1 - rate)\n",
    "        \n",
    "        self.cache['output_dropout'] = z\n",
    "        # --- ----------- --- #\n",
    "        \n",
    "        # --- Output --- #\n",
    "        return(z)\n",
    "        \n",
    "    def FullyConnected(self, x, filters, activation='softmax', weight='he'):\n",
    "        \"\"\"'1D-MaxPooling'\n",
    "           Args:\n",
    "               x = input data\n",
    "               filters = number of classes\n",
    "               activation = default is 'softmax'\n",
    "               weight = initial weight is based on 'he weights'\n",
    "        \"\"\"\n",
    "        # --- Calculation --- #\n",
    "        x = x.flatten().reshape(-1, 1)\n",
    "        self.cache['input_fc'] = x\n",
    "        \n",
    "        w, b = self.HeWeights(x.shape[0], filters)\n",
    "        self.cache[\"weight_fc\"] = w\n",
    "        self.cache[\"bias_fc\"] = b\n",
    "        \n",
    "        z = np.dot(w, x) + b\n",
    "        \n",
    "        self.cache['output_fc'] = z\n",
    "        \n",
    "        z = Activation(z).Softmax()\n",
    "        self.cache['activation_fc'] = z\n",
    "        # --- ----------- --- #\n",
    "        \n",
    "        # --- Output --- #\n",
    "        return(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.Convolution(x, filters=256)\n",
    "        x = self.MaxPooling(x, pool_size=2, stride=1)\n",
    "        x = self.Dropout(x, rate=0.1)\n",
    "        x = self.FullyConnected(x, filters=10)\n",
    "        return(x)\n",
    "        \n",
    "    def Backpropagation(self, y_true, y_pred, loss='cross_entropy', optimizer='sgd', learning_rate=1e-3):\n",
    "        \"\"\"Backward pass through the network.\"\"\"\n",
    "        conv_filters = 256\n",
    "        conv_stride = 1\n",
    "        kernel_size = 3\n",
    "        l = Loss(y_true, y_pred).CategoricalCrossEntropy()\n",
    "        \n",
    "        # Output Layer Gradient (Softmax + Cross-Entropy)\n",
    "        delta_fc = l\n",
    "        dw_fc = np.dot(delta_fc, self.cache['input_fc'].T)\n",
    "        db_fc = delta_fc\n",
    "        \n",
    "        d_dropout = np.dot(self.cache[\"weight_fc\"].T, delta_fc)\n",
    "        d_pool = d_dropout.reshape(self.cache[\"input_maxpool\"].shape)\n",
    "        d_pool *= self.cache['output_dropout'] / 0.9\n",
    "        \n",
    "        d_relu = np.zeros_like(self.cache['activation_conv'])\n",
    "        for i in range(conv_filters):\n",
    "            for j in range(self.cache['input_maxpool'].shape[1]):\n",
    "                d_relu[i, j:j+2] += d_pool[i, j] * self.cache['output_maxpool'][i, j:j+2]\n",
    "                \n",
    "        d_z_conv = d_relu * (self.cache['output_conv'] > 0)\n",
    "        \n",
    "        dw_conv = np.zeros_like(self.cache['weight_conv'])\n",
    "        db_conv = np.sum(d_z_conv, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        x = self.cache['input_conv']\n",
    "        for i in range(conv_filters):\n",
    "            for j in range(d_z_conv.shape[1]):\n",
    "                segment = x[j*conv_stride : j*conv_stride + kernel_size]\n",
    "                dw_conv[i] += d_z_conv[i, j] * segment\n",
    "        \n",
    "        \n",
    "        params = [self.cache['weight_fc'], self.cache['bias_fc'], self.cache['weight_conv'], self.cache['bias_conv']]\n",
    "        grads = [dw_fc, db_fc, dw_conv, db_conv]\n",
    "        optimizer = Optimizer(params, grads)\n",
    "        \n",
    "        backpropagated_result = optimizer.SGD()\n",
    "        \n",
    "        return(backpropagated_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3.10",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
