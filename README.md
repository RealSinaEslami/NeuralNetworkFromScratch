# NeuralNetworkFromScratch
An implementation of Neural Network from scratch on mnist dataset
__________
The code i've provided is not complete like Pytorch and Tensorflow at all! It's just a simple two layer network. But i made it clear for understanding. 

You have to know only algebra and math and a little bit working with numpy library.
Consider that my code is not complete and efficient and the backpropagation function may not work properly. I only provide it for better learning ðŸ˜Š

how does the network looke like?
* 1. Forward Function âœ…
* 2. Backward Function (Backpropagation) âœ…
* 3. Convolution âœ…
* 4. Pooling (MaxPooling) âœ…
* 5. Dropout âœ…
* 6. Loss (Cross-Entropy) âœ…
* 7. Optimizer (SGD) âœ…
    * SGD is simpler to implement
* 8. Fully Connected (Dense or Linear) âœ…
* 9. Activation (ReLU & Sigmoid) âœ…
__________
You can use my handwritten notes for better understanding

![NN from Scratch](https://github.com/user-attachments/assets/9ffb208a-bc2f-4d06-9a9a-7e2d4ec12ece)
